---

# renovate: datasource=github-releases depName=ollama/ollama
ollama_github_version: 'v0.12.11'
ollama_url: "https://github.com/ollama/ollama/releases/download/{{ ollama_github_version }}/ollama-linux-amd64.tgz"
# ollama_hash: "sha256:1d1aea4dcae1d6b736141c2998416a742b89d7fda9321d60db7ea286db77268d"
ollama_user: _ollama
ollama_home: /var/_ollama
ollama_user_groups: ['render', 'video']

ollama_models_downloads:
  - tinyllama  # 700MB
  # - mistral  # 4GB
  # - deepseek-r1  # 5GB
  # - phi4  # 9GB
  # - llama3.3  # 43GB

ollama_amdgpu_enable: false

ollama_env_variables_template: "default-env.j2"
ollama_env_config: []
# OLLAMA_HOST=
# OLLAMA_DEBUG=1
# https://github.com/ollama/ollama/blob/main/docs/gpu.md#overrides
# HSA_OVERRIDE_GFX_VERSION=

# This will set ollama HTTPS_PROXY
# proxy_url: http://proxy:3128

# if using GPU, need those settings
# ollama_systemd_rwpaths: /dev/kfd /dev/dri
# ollama_systemd_privatedevices: no

ollama_cgroups_restriction_enable: true

ollama_reporting_template: ollama_report.sh
ollama_reporting_dest: /var/tmp
ollama_logrotate_days: 10

install_archives: "/var/_install"
is_container: false
